{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck.set_region('us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recobundles_hcp(params):\n",
    "    subject, hcp_ak, hcp_sk = params\n",
    "    import pandas as pd\n",
    "    import s3fs\n",
    "    import json\n",
    "    import logging\n",
    "    import boto3\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "    import dipy.data as dpd\n",
    "    import dipy.tracking.utils as dtu\n",
    "    import dipy.tracking.streamline as dts\n",
    "    from dipy.io.streamline import save_tractogram, load_tractogram\n",
    "    from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "    from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "    from dipy.io.stateful_tractogram import Space\n",
    "    import dipy.core.gradients as dpg\n",
    "    from dipy.segment.mask import median_otsu\n",
    "\n",
    "    import AFQ.data as afd\n",
    "    import AFQ.tractography as aft\n",
    "    import AFQ.registration as reg\n",
    "    import AFQ.dti as dti\n",
    "    import AFQ.segmentation as seg\n",
    "    from AFQ import api\n",
    "    from AFQ import csd\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    log = logging.getLogger(__name__)    \n",
    "    \n",
    "    log.info(f\"Fetching HCP subject {subject}\")\n",
    "    afd.fetch_hcp([subject], \n",
    "                  profile_name=False,\n",
    "                  aws_access_key_id=hcp_ak,\n",
    "                  aws_secret_access_key=hcp_sk)    \n",
    "        \n",
    "    dwi_dir = op.join(afd.afq_home, 'HCP', 'derivatives',\n",
    "                      'dmriprep', f'sub-{subject}', 'sess-01/dwi')\n",
    "\n",
    "    hardi_fdata = op.join(dwi_dir, f\"sub-{subject}_dwi.nii.gz\")\n",
    "    hardi_fbval = op.join(dwi_dir, f\"sub-{subject}_dwi.bval\")\n",
    "    hardi_fbvec = op.join(dwi_dir, f\"sub-{subject}_dwi.bvec\")\n",
    "\n",
    "    log.info(f\"Reading data from file {hardi_fdata}\")\n",
    "    img = nib.load(hardi_fdata)\n",
    "    log.info(f\"Creating gradient table from {hardi_fbval} and {hardi_fbvec}\")\n",
    "    gtab = dpg.gradient_table(hardi_fbval, hardi_fbvec)\n",
    "\n",
    "    client = boto3.resource('s3')\n",
    "    bucket_name = 'hcp.recobundles'\n",
    "    b = client.Bucket(bucket_name)\n",
    "    \n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    brain_mask_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_brain_mask.nii.gz'\n",
    "    if fs.exists(brain_mask_fname):\n",
    "        log.info(f\"Brain-mask exists. Reading from {brain_mask_fname}\")\n",
    "        be_img = afd.s3fs_nifti_read(brain_mask_fname)\n",
    "        brain_mask = be_img.get_data()\n",
    "    else:\n",
    "        log.info(\"Calculating brain-mask\")\n",
    "        mean_b0 = np.mean(img.get_data()[..., gtab.b0s_mask], -1)\n",
    "        _, brain_mask = median_otsu(mean_b0, median_radius=4,\n",
    "                                    numpass=1, autocrop=False,\n",
    "                                    vol_idx=None, dilate=10)\n",
    "        be_img = nib.Nifti1Image(brain_mask.astype(int),\n",
    "                                img.affine)\n",
    "        log.info(f\"Saving to {brain_mask_fname}\")\n",
    "        afd.s3fs_nifti_write(be_img, brain_mask_fname)\n",
    "        \n",
    "    \n",
    "    fa_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_dti_FA.nii.gz'\n",
    "    dti_params_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_dti.nii.gz'\n",
    "    dti_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_dti.json'\n",
    "    if fs.exists(fa_fname):\n",
    "        log.info(f\"DTI already exists. Reading FA from {fa_fname}\")\n",
    "        log.info(f\"DTI already exists. Reading params from {dti_params_fname}\")\n",
    "        FA_img = afd.s3fs_nifti_read(fa_fname)\n",
    "        dti_params = afd.s3fs_nifti_read(dti_params_fname)\n",
    "    else:\n",
    "        log.info(\"Calculating DTI\")\n",
    "        dti_params = dti.fit_dti(hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                out_dir='.', b0_threshold=50,\n",
    "                                mask=brain_mask)\n",
    "        FA_img = nib.load('./dti_FA.nii.gz')\n",
    "        log.info(f\"Writing FA to {fa_fname}\")\n",
    "        afd.s3fs_nifti_write(FA_img, fa_fname)\n",
    "        dti_params_img = nib.load('./dti_params.nii.gz')\n",
    "        log.info(f\"Writing DTI params to {dti_params_fname}\")\n",
    "        afd.s3fs_nifti_write(dti_params_img, dti_params_fname)\n",
    "#         dti_params_json = {\"Model\": \"Diffusion Tensor\",\n",
    "#                            \"OrientationRepresentation\": \"param\",\n",
    "#                             \"ReferenceAxes\": \"xyz\",\n",
    "#                             \"Parameters\": {\n",
    "#                                 \"FitMethod\": \"ols\",\n",
    "#                                 \"OutlierRejection\": False\n",
    "#                                 }\n",
    "#                           }\n",
    "#         log.info(f\"Writing DTI metadata to {dti_meta_fname}\")\n",
    "#         with fs.open(dti_meta_fname, 'w') as ff:\n",
    "#             json.dump(ff, dti_params_json)\n",
    "\n",
    "    log.info(f\"Reading FA data from img\")\n",
    "    FA_data = FA_img.get_fdata()\n",
    "\n",
    "#    log.info(\"Getting the MNI template\")   \n",
    "#     MNI_T2_img = afd.s3fs_nifti_read('hcp.recobundles/mni_icbm152_t2_tal_nlin_asym_09a.nii.gz')\n",
    "#     mapping_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_mapping.nii.gz'\n",
    "#     if fs.exists(mapping_fname):\n",
    "#         log.info(f\"Mapping already exists. Getting it from {mapping_fname}\")\n",
    "#         fs.download(mapping_fname, './mapping.nii.gz')\n",
    "#         log.info(f\"Reading mapping from './mapping.nii.gz'\")\n",
    "#         mapping = reg.read_mapping('./mapping.nii.gz', img, MNI_T2_img)\n",
    "#     else:\n",
    "#         log.info(f\"Creating mapping.\")\n",
    "#         gtab = dpg.gradient_table(hardi_fbval, hardi_fbvec)\n",
    "#         log.info(f\"Calculating SyN registration.\")\n",
    "#         warped_hardi, mapping = reg.syn_register_dwi(hardi_fdata, gtab,\n",
    "#                                                     template=MNI_T2_img)\n",
    "#         log.info(f\"Writing to './mapping.nii.gz'\")\n",
    "#         reg.write_mapping(mapping, './mapping.nii.gz')\n",
    "#         log.info(f\"Uploading to {mapping_fname}\")\n",
    "#         fs.upload('mapping.nii.gz', mapping_fname)\n",
    "\n",
    "    csd_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_csd.nii.gz'\n",
    "    csd_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_csd.json'\n",
    "\n",
    "    if fs.exists(csd_fname):\n",
    "        log.info(f\"CSD already exists. Getting it from {csd_fname}\")        \n",
    "        csd_params = afd.s3fs_nifti_read(csd_fname)\n",
    "    else:\n",
    "        log.info(f\"Calculating CSD\")        \n",
    "        csd_params = csd.fit_csd(hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                 out_dir='.', b0_threshold=50,\n",
    "                                 mask=brain_mask)\n",
    "        afd.s3fs_nifti_write(nib.load(csd_params), csd_fname)\n",
    "\n",
    "        \n",
    "#         csd_params_json = {\n",
    "#     \"Model\": \"Constrained Spherical Deconvolution (CSD)\",\n",
    "#     \"ModelURL\": \"https://github.com/nipy/dipy/commit/abf31d15a0ee5dc0704ee03ebbba57358d540612\",\n",
    "#     \"Shells\": [ 0, 1000, 2000, 3000 ],\n",
    "#     \"Parameters\": {\n",
    "#         \"ResponseFunctionTensor\" : \"auto\",\n",
    "#         \"SphericalHarmonicBasis\": \"Descoteaux\",\n",
    "#         \"NonNegativityConstraint\": \"hard\",\n",
    "#         \"SphericalHarmonicDegree\" : 8\n",
    "#                 }\n",
    "#             }\n",
    "        \n",
    "#         log.info(f\"Writing CSD metadata to {csd_meta_fname}\")\n",
    "#         with fs.open(csd_meta_fname, 'w') as ff:\n",
    "#             json.dump(ff, csd_params_json)\n",
    "\n",
    "\n",
    "    csd_streamlines_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det.trk'\n",
    "    csd_streamlines_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det.json'\n",
    "    if fs.exists(csd_streamlines_fname):\n",
    "        log.info(f\"Streamlines already exist. Loading from {csd_streamlines_fname}\")        \n",
    "        fs.download(csd_streamlines_fname, './csd_streamlines.trk')\n",
    "        tg = load_tractogram('./csd_streamlines.trk', img)\n",
    "        streamlines = tg.streamlines\n",
    "    else:\n",
    "        log.info(f\"Generating streamlines\")      \n",
    "        seed_roi = np.zeros(img.shape[:-1])\n",
    "        seed_roi[FA_data > 0.4] = 1\n",
    "        streamlines = aft.track(csd_params, seed_mask=seed_roi,\n",
    "                                directions='det', stop_mask=FA_data,\n",
    "                                stop_threshold=0.1)\n",
    "        log.info(f\"After tracking, there are {len(streamlines)} streamlines\")\n",
    "        sft = StatefulTractogram(streamlines, img, Space.RASMM)\n",
    "        save_tractogram(sft, './csd_streamlines.trk',\n",
    "                        bbox_valid_check=False)\n",
    "        log.info(f\"Uploading streamlines to {csd_streamlines_fname}\")\n",
    "        fs.upload('./csd_streamlines.trk', csd_streamlines_fname)\n",
    "#         csd_streamlines_json = {\n",
    "#             \"Algorithm\" : \"LocalTracking\",\n",
    "#             \"AlgorithmURL\":\"https://github.com/yeatmanlab/pyAFQ/commit/c04835cd4ca13d28c20bb449d6f088e656c55e57\",\n",
    "#             \"Parameters\":{\n",
    "#             \"SeedRoi\": \"dti_FA>0.4\",\n",
    "#             \"Directions\": \"det\",\n",
    "#             \"StopMask\" : \"dti_FA<0.1\"}\n",
    "#             }\n",
    "#         log.info(f\"Writing streamlines metadata to {csd_streamlines_meta_fname}\")\n",
    "#         with fs.open(csd_streamlines_meta_fname, 'w') as ff:\n",
    "#             json.dump(ff, csd_streamlines_json)\n",
    "        \n",
    "    log.info(\"Segmenting\")\n",
    "        \n",
    "    bundle_names = ['CST',\n",
    "                    'C',\n",
    "                    'F',\n",
    "                    'UF',\n",
    "                    'MCP',\n",
    "                    'AF',\n",
    "                    'CCMid',\n",
    "                    'AF',\n",
    "                    'CC_ForcepsMajor',\n",
    "                    'CC_ForcepsMinor',\n",
    "                    'IFOF']\n",
    "\n",
    "    bundles = api.make_bundle_dict(bundle_names=bundle_names, seg_algo=\"reco\")\n",
    "\n",
    "    segmentation = seg.Segmentation(algo='reco',\n",
    "                                    model_clust_thr=20,\n",
    "                                    reduction_thr=20,\n",
    "                                    b0_threshold=50,\n",
    "                                    return_idx=True)\n",
    "    segmentation.segment(bundles, streamlines)\n",
    "    fiber_groups = segmentation.fiber_groups\n",
    "\n",
    "    sl_count = []\n",
    "    for kk in fiber_groups:\n",
    "        sl_count.append(len(fiber_groups[kk]['sl']))\n",
    "        log.info(f\"There are {sl_count[-1]} streamlines in {kk}\")\n",
    "        sft = StatefulTractogram(fiber_groups[kk]['sl'], img, Space.RASMM)\n",
    "        local_tg_fname = './%s_reco.trk'%kk\n",
    "        save_tractogram(sft, local_tg_fname,\n",
    "                        bbox_valid_check=False)\n",
    "        tg_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-recobundles_bundle-{kk}.trk'\n",
    "        log.info(f\"Uploading {local_tg_fname} to {tg_fname}\")\n",
    "        fs.upload('./%s_reco.trk'%kk, tg_fname)\n",
    "#        tg_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-recobundles_bundle-{kk}.json'\n",
    "#         tg_meta_json = {\n",
    "#             \"Algorithm\" : \"RecoBundles\",\n",
    "#             \"AlgorithmURL\" : \"https://github.com/yeatmanlab/pyAFQ/commit/871c7d567e83fae5041d67802fc8ec03791a877e\",\n",
    "#             \"Parameters\":\n",
    "#             {\"model_clust_thr\":20,\n",
    "#              \"reduction_thr\":20}\n",
    "#         }\n",
    "#         log.info(f\"Uploading segmentation metadata to {tg_meta_fname}\")\n",
    "#         with fs.open(tg_meta_fname, 'w') as ff:\n",
    "#             json.dump(ff, tg_meta_json)\n",
    "\n",
    "        np.save('bundle_idx.npy', fiber_groups[kk]['idx'])\n",
    "        idx_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-recobundles_bundle-{kk}_idx.npy'\n",
    "        log.info(f\"Uploading bundle indices to {idx_fname}\")\n",
    "        fs.upload('bundle_idx.npy', idx_fname)\n",
    "\n",
    "    log.info(\"Saving streamline counts\")\n",
    "    sl_count = pd.DataFrame(data=sl_count, index=fiber_groups.keys(), columns=[\"streamlines\"])\n",
    "    sl_count.to_csv(\"./sl_count.csv\")\n",
    "    sl_count_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-recobundles_counts.csv'\n",
    "    log.info(f\"Uploading streamline counts to {sl_count_fname}\")\n",
    "    fs.upload(\"./sl_count.csv\", sl_count_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP = configparser.ConfigParser()\n",
    "CP.read_file(open(op.join(op.expanduser('~'), '.aws', 'credentials')))\n",
    "CP.sections()\n",
    "ak = CP.get('hcp', 'AWS_ACCESS_KEY_ID')\n",
    "sk = CP.get('hcp', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_knot = ck.Knot(name='recobundles_hcp-64gb-22',\n",
    "                  func=recobundles_hcp,\n",
    "                  image_github_installs=\"https://github.com/arokem/pyAFQ.git@779e0d0c76c0d729387129fcc4437c25f6ad6d7d\",\n",
    "                  pars_policies=('AmazonS3FullAccess',),\n",
    "                  resource_type=\"SPOT\",\n",
    "                  bid_percentage=100,\n",
    "                  memory=64000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [(sub, ak, sk) for sub in [100307, 100206, 970764]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = rb_knot.map(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j0 = rb_knot.jobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j0.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rb_knot.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
