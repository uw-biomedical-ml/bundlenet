{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck.set_region('us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP = configparser.ConfigParser()\n",
    "CP.read_file(open(op.join(op.expanduser('~'), '.aws', 'credentials')))\n",
    "CP.sections()\n",
    "ak = CP.get('hcp', 'AWS_ACCESS_KEY_ID')\n",
    "sk = CP.get('hcp', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recobundles_hcp(params):\n",
    "    subject, hcp_ak, hcp_sk = params\n",
    "    import pandas as pd\n",
    "    import s3fs\n",
    "    import logging\n",
    "    import boto3\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "    import dipy.data as dpd\n",
    "    import dipy.tracking.utils as dtu\n",
    "    import dipy.tracking.streamline as dts\n",
    "    from dipy.io.streamline import save_tractogram, load_tractogram\n",
    "    from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "    from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "    from dipy.io.stateful_tractogram import Space\n",
    "    import dipy.core.gradients as dpg\n",
    "    from dipy.segment.mask import median_otsu\n",
    "\n",
    "    import AFQ.data as afd\n",
    "    import AFQ.tractography as aft\n",
    "    import AFQ.registration as reg\n",
    "    import AFQ.dti as dti\n",
    "    import AFQ.segmentation as seg\n",
    "    from AFQ import api\n",
    "    from AFQ import csd\n",
    "    \n",
    "\n",
    "    log = logging.Logger(__name__)    \n",
    "    \n",
    "    log.info(f\"Fetching HCP subject {subject}\")\n",
    "    afd.fetch_hcp([subject], \n",
    "                  aws_access_key_id=hcp_ak,\n",
    "                  aws_secret_access_key=hcp_sk)    \n",
    "        \n",
    "    dwi_dir = op.join(afd.afq_home, 'HCP', 'derivatives',\n",
    "                      'dmriprep', f'sub-{subject}', 'sess-01/dwi')\n",
    "\n",
    "    hardi_fdata = op.join(dwi_dir, f\"sub-{subject}_dwi.nii.gz\")\n",
    "    hardi_fbval = op.join(dwi_dir, f\"sub-{subject}_dwi.bval\")\n",
    "    hardi_fbvec = op.join(dwi_dir, f\"sub-{subject}_dwi.bvec\")\n",
    "\n",
    "    log.info(f\"Reading data from file {hardi_fdata}\")\n",
    "    img = nib.load(hardi_fdata)\n",
    "    log.info(f\"Creating gradient table from {hardi_fbval} and {hardi_fbvec}\")\n",
    "    gtab = dpg.gradient_table(hardi_fbval, hardi_fbvec)\n",
    "\n",
    "    client = boto3.resource('s3')\n",
    "    bucket_name = 'hcp.recobundles'\n",
    "    b = client.Bucket(bucket_name)\n",
    "    \n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    brain_mask_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_brain_mask.nii.gz'\n",
    "    if fs.exists(brain_mask_fname):\n",
    "        log.info(f\"Brain-mask exists. Reading from {brain_mask_fname}\")\n",
    "        be_img = afd.s3fs_nifti_read(brain_mask_fname)\n",
    "        brain_mask = be_img.get_data()\n",
    "    else:\n",
    "        log.info(\"Calculating brain-mask\")\n",
    "        mean_b0 = np.mean(img.get_data()[..., gtab.b0s_mask], -1)\n",
    "        _, brain_mask = median_otsu(mean_b0, median_radius=4,\n",
    "                                    numpass=1, autocrop=False,\n",
    "                                    vol_idx=None, dilate=10)\n",
    "        be_img = nib.Nifti1Image(brain_mask.astype(int),\n",
    "                                img.affine)\n",
    "        log.info(f\"Saving to {brain_mask_fname}\")\n",
    "        afd.s3fs_nifti_write(be_img, brain_mask_fname)\n",
    "        \n",
    "    \n",
    "    fa_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_dti_FA.nii.gz'\n",
    "    dti_params_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_dti.nii.gz'\n",
    "    dti_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_dti.json'\n",
    "    if fs.exists(fa_fname):\n",
    "        log.info(f\"DTI already exists. Reading FA from {fa_fname}\")\n",
    "        log.info(f\"DTI already exists. Reading params from {dti_params_fname}\")\n",
    "        FA_img = afd.s3fs_nifti_read(fa_fname)\n",
    "        dti_params = afd.s3fs_nifti_read(dti_params_fname)\n",
    "    else:\n",
    "        log.info(\"Calculating DTI\")\n",
    "        dti_params = dti.fit_dti(hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                out_dir='.', b0_threshold=50,\n",
    "                                mask=brain_mask)\n",
    "        FA_img = nib.load('./dti_FA.nii.gz')\n",
    "        log.info(f\"Writing FA to {fa_fname}\")\n",
    "        afd.s3fs_nifti_write(FA_img, fa_fname)\n",
    "        dti_params_img = nib.load('./dti_params.nii.gz')\n",
    "        log.info(f\"Writing DTI params to {dti_params_fname}\")\n",
    "        afd.s3fs_nifti_write(dti_params_img, dti_params_fname)\n",
    "        dti_params_json = {\"Model\": \"Diffusion Tensor\",\n",
    "                           \"OrientationRepresentation\": \"param\",\n",
    "                            \"ReferenceAxes\": \"xyz\",\n",
    "                            \"Parameters\": {\n",
    "                                \"FitMethod\": \"ols\",\n",
    "                                \"OutlierRejection\": False\n",
    "                                }\n",
    "                          }\n",
    "        log.info(f\"Writing DTI metadata to {dti_meta_fname}\")\n",
    "        afd.s3fs_write_json(dti_params_json, dti_meta_fname)\n",
    "\n",
    "    log.info(f\"Reading FA data from img\")\n",
    "    FA_data = FA_img.get_fdata()\n",
    "\n",
    "    log.info(\"Getting the MNI template\")\n",
    "    \n",
    "    MNI_T2_img = afd.s3fs_nifti_read('hcp.recobundles/mni_icbm152_t2_tal_nlin_asym_09a.nii')\n",
    "    mapping_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_mapping.nii.gz'\n",
    "    if fs.exists('mapping.nii.gz'):\n",
    "        log.info(f\"Mapping already exists. Getting it from {mapping_fname}\")\n",
    "        fs.download(mapping_fname, './mapping.nii.gz')\n",
    "        log.info(f\"Reading mapping from './mapping.nii.gz'\")\n",
    "        mapping = reg.read_mapping('./mapping.nii.gz', img, MNI_T2_img)\n",
    "    else:\n",
    "        log.info(f\"Creating mapping.\")\n",
    "        gtab = dpg.gradient_table(hardi_fbval, hardi_fbvec)\n",
    "        log.info(f\"Calculating SyN registration.\")\n",
    "        warped_hardi, mapping = reg.syn_register_dwi(hardi_fdata, gtab,\n",
    "                                                    template=MNI_T2_img)\n",
    "        log.info(f\"Writing to './mapping.nii.gz'\")\n",
    "        reg.write_mapping(mapping, './mapping.nii.gz')\n",
    "        log.info(f\"Uploading to {mapping_fname}\")\n",
    "        fs.upload(mapping.nii.gz, mapping_fname)\n",
    "        \n",
    "    bundle_names = ['CST',\n",
    "                    'C',\n",
    "                    'F',\n",
    "                    'UF',\n",
    "                    'MCP',\n",
    "                    'AF',\n",
    "                    'CCMid',\n",
    "                    'AF',\n",
    "                    'CC_ForcepsMajor',\n",
    "                    'CC_ForcepsMinor',\n",
    "                    'IFOF']\n",
    "\n",
    "    bundles = api.make_bundle_dict(bundle_names=bundle_names, seg_algo=\"reco\")\n",
    "\n",
    "\n",
    "    csd_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_csd.nii.gz'\n",
    "    csd_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_csd.json'\n",
    "\n",
    "    if fs.exists(csd_fname):\n",
    "        log.info(f\"CSD already exists. Getting it from {csd_fname}\")        \n",
    "        csd_params = afd.s3fs_nifti_read(csd_fname)\n",
    "    else:\n",
    "        log.info(f\"Calculating CSD\")        \n",
    "        csd_params = csd.fit_csd(hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                 out_dir='.', b0_threshold=50,\n",
    "                                 mask=brain_mask)\n",
    "        \n",
    "        csd_params_json = {\n",
    "    \"Model\": \"Constrained Spherical Deconvolution (CSD)\",\n",
    "    \"ModelURL\": \"https://github.com/nipy/dipy/commit/abf31d15a0ee5dc0704ee03ebbba57358d540612\",\n",
    "    \"Shells\": [ 0, 1000, 2000, 3000 ],\n",
    "    \"Parameters\": {\n",
    "        \"ResponseFunctionTensor\" : \"auto\",\n",
    "        \"SphericalHarmonicBasis\": \"Descoteaux\",\n",
    "        \"NonNegativityConstraint\": \"hard\",\n",
    "        \"SphericalHarmonicDegree\" : 8\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        log.info(f\"Writing CSD metadata to {csd_meta_fname}\")\n",
    "        afd.s3fs_write_json(csd_params_json, csd_meta_fname)\n",
    "\n",
    "\n",
    "    csd_streamlines_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det.trk'\n",
    "    csd_streamlines_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det.json'\n",
    "    if fs.exists(csd_streamlines_fname):\n",
    "        log.info(f\"Streamlines already exist. Loading from {csd_streamlines_fname}\")        \n",
    "        fs.download(csd_streamlines_fname, '/csd_streamlines.trk')\n",
    "        tg = load_tractogram('./csd_streamlines.trk', img)\n",
    "        streamlines = tg.streamlines\n",
    "    else:\n",
    "        log.info(f\"Generating streamlines\")      \n",
    "        seed_roi = np.zeros(img.shape[:-1])\n",
    "        seed_roi[FA_data > 0.4] = 1\n",
    "        streamlines = aft.track(csd_params, seed_mask=seed_roi,\n",
    "                                directions='det', stop_mask=FA_data,\n",
    "                                stop_threshold=0.1)\n",
    "        log.info(f\"After tracking, there are {len(streamlines)} streamlines\")\n",
    "        sft = StatefulTractogram(streamlines, img, Space.RASMM)\n",
    "        save_tractogram(sft, './csd_streamlines_reco.trk',\n",
    "                        bbox_valid_check=False)\n",
    "        log.info(f\"Uploading streamlines to {csd_streamlines_fname}\")\n",
    "        fs.upload('./csd_streamlines.trk')\n",
    "        csd_streamlines_json = {\n",
    "            \"Algorithm\" : \"LocalTracking\",\n",
    "            \"AlgorithmURL\":\"https://github.com/yeatmanlab/pyAFQ/commit/c04835cd4ca13d28c20bb449d6f088e656c55e57\",\n",
    "            \"Parameters\":{\n",
    "            \"SeedRoi\": \"dti_FA>0.4\",\n",
    "            \"Directions\": \"det\",\n",
    "            \"StopMask\" : \"dti_FA<0.1\"}\n",
    "            }\n",
    "        log.info(f\"Writing streamlines metadata to {csd_streamlines_meta_fname}\")\n",
    "        afd.s3fs_write_json(csd_streamlines_json, csd_streamlines_meta_fname)\n",
    "        \n",
    "    log.info(\"Segmenting\")\n",
    "    segmentation = seg.Segmentation(algo='reco',\n",
    "                                    model_clust_thr=20,\n",
    "                                    reduction_thr=20)\n",
    "    segmentation.segment(bundles, streamlines)\n",
    "    fiber_groups = segmentation.fiber_groups\n",
    "\n",
    "    sl_count = []\n",
    "    for kk in fiber_groups:\n",
    "        sl_count.append(len(fiber_groups[kk]))\n",
    "        log.info(f\"There are {len(fiber_groups[kk])} streamlines in {kk}\")\n",
    "        sft = StatefulTractogram(fiber_groups[kk], img, Space.RASMM)\n",
    "        local_tg_fname = './%s_reco.trk'%kk\n",
    "        save_tractogram(sft, local_tg_fname,\n",
    "                        bbox_valid_check=False)\n",
    "        tg_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-recobundles_bundle-{kk}.trk'\n",
    "        log.info(f\"Uploading {local_tg_fname} to {tg_fname}\")\n",
    "        fs.upload('./%s_reco.trk'%kk, tg_fname)\n",
    "        tg_meta_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-recobundles_bundle-{kk}.json'\n",
    "        tg_meta_json = {\n",
    "            \"Algorithm\" : \"RecoBundles\",\n",
    "            \"AlgorithmURL\" : \"https://github.com/yeatmanlab/pyAFQ/commit/871c7d567e83fae5041d67802fc8ec03791a877e\",\n",
    "            \"Parameters\":\n",
    "            {\"model_clust_thr\":20,\n",
    "             \"reduction_thr\":20}\n",
    "        }\n",
    "        log.info(f\"Uploading segmentation metadata to {tg_meta_fname}\")\n",
    "        afd.s3fs_write_json(tg_meta_json, tg_meta_fname)\n",
    "    \n",
    "    log.info(\"Saving streamline counts\")\n",
    "    sl_count = pd.DataFrame(data=lengths, index=fiber_groups.keys(), columns=[\"streamlines\"])\n",
    "    sl_count.to_csv(\"./sl_count.csv\")\n",
    "    sl_count_fname = f'hcp.recobundles/sub-{subject}/sub-{subject}_model-csd_track-det_segment-recobundles_counts.csv'\n",
    "    log.info(f\"Uploading streamline counts to {sl_count_fname}\")\n",
    "    fs.upload(\"./sl_count.csv\", sl_count_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ck.DockerImage(func=recobundles_hcp, github_installs=\"https://github.com/arokem/pyAFQ.git@recobundles_hcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "CloudknotInputError",
     "evalue": "The requested bucket name already exists and you do not have permission to put or get objects in it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBucketAlreadyExists\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/cloudknot/aws/base_classes.py\u001b[0m in \u001b[0;36mset_s3_params\u001b[0;34m(bucket, policy, sse)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 clients['s3'].create_bucket(\n\u001b[0;32m--> 246\u001b[0;31m                     \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                 )\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBucketAlreadyExists\u001b[0m: An error occurred (BucketAlreadyExists) when calling the CreateBucket operation: The requested bucket name is not available. The bucket namespace is shared by all users of the system. Please select a different name and try again.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/cloudknot/aws/base_classes.py\u001b[0m in \u001b[0;36mtest_bucket_put_get\u001b[0;34m(bucket_, sse_)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCloudknotInputError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-696a543c91ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mpars_policies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AmazonS3FullAccess'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mresource_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SPOT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                   bid_percentage=100)\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/cloudknot/cloudknot.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, pars, pars_policies, docker_image, base_image, func, image_script_path, image_work_dir, image_github_installs, username, repo_name, image_tags, job_definition_name, job_def_vcpus, memory, retries, compute_environment_name, instance_types, resource_type, min_vcpus, max_vcpus, desired_vcpus, image_id, ec2_key_pair, bid_percentage, job_queue_name, priority)\u001b[0m\n\u001b[1;32m   1273\u001b[0m             }\n\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars_cleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pars'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_docker_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_docker_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_cleanup\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/cloudknot/cloudknot.py\u001b[0m in \u001b[0;36mset_pars\u001b[0;34m(knot_name, input_pars, pars_policies_)\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                         \u001b[0mpars_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpars_policies_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0maws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCannotCreateResourceException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m                         pars_ = Pars(name=knot_name,\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/cloudknot/cloudknot.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, batch_service_role_name, ecs_instance_role_name, spot_fleet_role_name, policies, use_default_vpc, ipv4_cidr, instance_tenancy)\u001b[0m\n\u001b[1;32m    285\u001b[0m                                 for policy in policy_names]\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0ms3_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_s3_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             policy_list = [s3_params.policy_arn] + [\n\u001b[1;32m    289\u001b[0m                 \u001b[0mpolicy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_arns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/cloudknot/aws/base_classes.py\u001b[0m in \u001b[0;36mget_s3_params\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Use set_s3_params to check for name availability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# and write to config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mset_s3_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/cloudknot/aws/base_classes.py\u001b[0m in \u001b[0;36mset_s3_params\u001b[0;34m(bucket, policy, sse)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucketAlreadyExists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mtest_bucket_put_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Check for Illegal Location Constraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/bundlenet/lib/python3.7/site-packages/cloudknot/aws/base_classes.py\u001b[0m in \u001b[0;36mtest_bucket_put_get\u001b[0;34m(bucket_, sse_)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClientError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             raise CloudknotInputError('The requested bucket name already '\n\u001b[0m\u001b[1;32m    226\u001b[0m                                       \u001b[0;34m'exists and you do not have permission '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                                       'to put or get objects in it.')\n",
      "\u001b[0;31mCloudknotInputError\u001b[0m: The requested bucket name already exists and you do not have permission to put or get objects in it."
     ]
    }
   ],
   "source": [
    "rb_knot = ck.Knot(name='recobundles_hcp',\n",
    "                  docker_image=image, \n",
    "                  pars_policies=('AmazonS3FullAccess',),\n",
    "                  resource_type=\"SPOT\",\n",
    "                  bid_percentage=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
